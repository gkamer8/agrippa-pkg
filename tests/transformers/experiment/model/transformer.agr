<model script-version="0.0.1">

    <!-- Uses default type, which is float32 -->
    <!-- For tokens: each row is a one hot vector, sequence proceeds vertically in the matrix -->

    <import from="tokens" dim="[var(ntokens), var(nvocab)]" />
    <import from="mask" dim="[var(ntokens), var(ntokens)]" />
    <import from="scale" dim="[1]" />
    <import from="posembedmatrix" dim="[var(ntokens), var(dmodel)]" />
    <import from="embed_scale" dim="[1]" />
    <import from="ln_eps" dim="[1]" />

    <block title="ShrunkEmbed">
        <node op="Identity">
            <params name="EmbedW" dim="[var(nvocab), var(dmodel)]" />
            <output name="Embed" />
        </node>
        <node op="Div">
            <input src="Embed" dim="[var(nvocab), var(dmodel)]" />
            <input src="embed_scale" />
            <output name="EmbedShrunk" />
        </node>
        <export from="EmbedShrunk" />
        <export from="Embed" />
    </block>

    <!-- Shrinks tokens into dmodel using a learned embedding -->
    <block title="Embedding">
        <import from="tokens" />
        <node op="MatMul" title="EmbedProjection">
            <input src="tokens" />
            <input src="EmbedShrunk" />
            <output name="embeddings" />
        </node>
        <export from="embeddings" dim="[var(ntokens), var(dmodel)]" />
    </block>

    <block title="PositionalEmbedding">
        <import from="embeddings" />
        <import from="posembedmatrix" />
        <node op="Add">
            <input src="embeddings" />
            <input src="posembedmatrix" />
            <output name="posembeddings" />
        </node>
        <export from="posembeddings" dim="[var(ntokens), var(dmodel)]" />
    </block>

    <!-- The big decoder block -->
    <!-- NOT IMPLEMENTED YET -->
    <block title="DecoderLayer" rep="var(nlayers)">
        <import from="posembeddings" dim="[var(ntokens), var(dmodel)]" />
        <import from="scale" />
        <import from="mask" dim="[var(ntokens), var(ntokens)]" />
	<import from="ln_eps" />

        <block title="Attention" stretch="var(nheads)">
            <import from="posembeddings" dim="[var(ntokens), var(dmodel)]" />
            <import from="scale" dim="[var(ntokens), var(ntokens)]" />
            <import from="mask" dim="[var(ntokens), var(ntokens)]" />
            <block title="LinearQKV">
                <import from="posembeddings" />
                <node op="MatMul">
                    <input src="posembeddings" />
                    <params name="QueryWeights" dim="[var(dmodel), var(dqueries)]" />
                    <output name="queries" dims="[var(ntokens), var(dqueries)]" />
                </node>
                <node op="MatMul">
                    <input src="posembeddings" />
                    <params name="KeyWeights" dim="[var(dmodel), var(dkeys)]" />
                    <output name="keys" dims="[var(ntokens), var(dkeys)]" />
                </node>
                <node op="MatMul">
                    <input src="posembeddings" />
                    <params name="ValueWeights" dim="[var(dmodel), var(dvalues)]" />
                    <output name="values" dim="[var(ntokens), var(dvalues)]" />
                </node>
                <export from="queries" />
                <export from="keys" />
                <export from="values" />
            </block>
            <block title="ScaledDotProductAttention">
                <import from="scale" dim="[var(ntokens), var(ntokens)]" />
                <import from="mask" dim="[var(ntokens), var(ntokens)]" />
                <import from="queries" />
                <import from="values" />
                <import from="keys" />
                <node op="Transpose">
                    <input src="keys" />
                    <output name="keys_t" dim="[var(dkeys), var(ntokens)]" />
                </node>
                <node op="MatMul">
                    <input src="queries" />
                    <input src="keys_t" />
                    <output name="matmul" dim="[var(ntokens), var(ntokens)]" />
                </node>
                <node op="Div">
                    <input src="matmul" />
                    <input src="scale" />
                    <output name="scaled" />
                </node>
                <node title="Mask" op="Add">
                    <input src="scaled" />
                    <input src="mask" />
                    <output name="masked" />
                </node>
                <node op="Softmax" axis="-1">
                    <input src="masked" />
                    <output name="softmaxed" />
                </node>
                <node op="MatMul" title="ValueMatmul">
                    <input src="softmaxed" />
                    <input src="values" />
                    <output name="attended" dim="[var(ntokens), var(dvalues)]" />
                </node>
            </block>            
            <export from="attended" />
        </block>

        <block title="ConcatLinear">
            <import from="attended" />
            <node op="MatMul">
                <input src="attended" />
                <params name="LinearConcatW" dim="[expr(dvalues * nheads), var(dmodel)]" />
                <output name="linear_concatenated" />
            </node>
            <export from="linear_concatenated" />
        </block>
        
        <block title="Add">
            <import from="linear_concatenated" />
            <import from="posembeddings" />
            <node op="Add">
                <input src="linear_concatenated" />
                <input src="posembeddings" />
                <output name="attended_added" />
            </node>
            <export from="attended_added" />
        </block>

        <block title="LayerNorm">
            <import from="attended_added" />
            <node op="ReduceMean" axes="[-1]">
                <input src="attended_added" />
                <output name="means" />
            </node>
            <node op="Sub">
                <input src="attended_added" />
                <input src="means" />
                <output name="centered" />
            </node>
            <node op="Mul">
                <input src="centered" />
                <input src="centered" />
                <output name="squared" />
            </node>
            <node op="ReduceMean" axes="[-1]">
                <input src="squared" />
                <output name="sigma_squared" />
            </node>
	    <node op="Add">
	    	 <input src="sigma_squared" />
		 <input src="ln_eps" />
		 <output name="stable" />
	    </node>
            <node op="Sqrt">
                <input src="stable" />
                <output name="sigma" />
            </node>
            <node op="Div">
                <params name="gain" dim="[1, var(dmodel)]" init="ones" />
                <input src="sigma" />
                <output name="gains" />
            </node>
            <node op="Mul">
                <input src="centered" />
                <input src="gains" />
                <output name="gained" />
            </node>
            <node op="Add">
                <input src="gained" />
                <params dim="[1, var(dmodel)]" name="biases" init="zeros" />
                <output name="layer_norm_out" />
            </node>
            <export from="layer_norm_out" />
        </block>

        <block title="FFN">
            <import from="layer_norm_out" />
            <node op="MatMul">
                <input src="layer_norm_out" />
                <params name="ffn_w" dim="[var(dmodel), var(dffnhidden)]" init="normal" init_args="[0, 0.0005]" />
                <output name="ffn_projected" dim="[var(ntokens), var(dffnhidden)]" />
            </node>
            <node op="Add" title="FFNBiases" >
                <input src="ffn_projected" />
                <!-- Note that the biases are numpy style broadcasted,
                        so the FFN remains identical for each position -->
                <params name="ffn_b" dim="[var(dffnhidden)]" init="zeros" />
                <output name="ffn_biased" />
            </node>
            <node op="Relu">
                <input src="ffn_biased" />
                <output name="ffn_relu" />
            </node>
            <node op="MatMul">
                <input src="ffn_relu" />
                <params name="ffn_w2" dim="[var(dffnhidden), var(dmodel)]" init="normal" init_args="[0, 0.0005]" />
                <output name="ffn_second_proj" />
            </node>
            <node op="Add" title="FFNBiases2">
                <input src="ffn_second_proj" />
                <params name="ffn_b2" dim="[var(dmodel)]" init="zeros" />
                <output name="ffn_out" />
            </node>
            <export from="ffn_out" />
        </block>

        <block title="Add2">
            <import from="ffn_out" />
            <import from="layer_norm_out" />
            <node op="Add">
                <input src="ffn_out" />
                <input src="layer_norm_out" />
                <output name="attended_added2" />
            </node>
            <export from="attended_added2" />
        </block>

        <block title="LayerNorm2">
            <import from="attended_added2" />
            <node op="ReduceMean" axes="[-1]">
                <input src="attended_added2" />
                <output name="means2" />
            </node>
            <node op="Sub">
                <input src="attended_added2" />
                <input src="means2" />
                <output name="centered2" />
            </node>
            <node op="Mul">
                <input src="centered2" />
                <input src="centered2" />
                <output name="squared2" />
            </node>
            <node op="ReduceMean" axes="[-1]">
                <input src="squared2" />
                <output name="sigma_squared2" />
            </node>
	    <node op="Add">
	    	 <input src="sigma_squared2" />
		 <input src="ln_eps" />
		 <output name="stable2" />
	    </node>
            <node op="Sqrt">
                <input src="stable2" />
                <output name="sigma2" />
            </node>
            <node op="Div">
                <params name="gain2" dim="[1, var(dmodel)]" init="ones" />
                <input src="sigma2" />
                <output name="gains2" />
            </node>
            <node op="Mul">
                <input src="centered2" />
                <input src="gains2" />
                <output name="gained2" />
            </node>
            <node op="Add">
                <input src="gained2" />
                <params dim="[1, var(dmodel)]" name="biases2" init="zeros" />
                <output name="layer_norm_out2" />
            </node>
            <export from="layer_norm_out2" />
        </block>

        <export from="layer_norm_out2" />
    </block>

    <!-- Linear to get to vocab size, no softmax - basically reverse of embedding -->

    <block title="GetRevEmbedWeights">
        <import from="Embed" />
        <node op="Transpose">
            <input src="Embed" />
            <output name="RevEmbed" />
        </node>
        <export from="RevEmbed" />
    </block>

    <block title="Linear">
        <import from="layer_norm_out2" />
         <node op="MatMul">
            <input src="layer_norm_out2" dim="[var(ntokens), var(dmodel)]" />
            <input src="RevEmbed" dim="[var(dmodel), var(nvocab)]" />
            <output name="end_linear" dim="[var(ntokens), var(nvocab)]" />
        </node>
        <export from="end_linear" />
    </block>

    <block title="EndSoftmax">
        <import from="end_linear" />
        <node op="Softmax" axis="-1">
            <input src="end_linear" dim="[var(ntokens), var(nvocab)]" />
            <output name="end_softmax" dim="[var(ntokens), var(nvocab)]" />
        </node>
        <export from="end_softmax" />
    </block>
    <export from="end_softmax" dim="[var(ntokens), var(nvocab)]" />
    <export from="end_linear" dim="[var(ntokens), var(nvocab)]" />
</model>