<model script-version="0.0.1">

    <!-- Uses default type, which is float32 -->

    <import from="tokens" dim="[var(nvocab), var(ntokens)]" />
    <import from="mask" dim="[var(ntokens), var(ntokens)]" />
    <import from="scale" dim="[1]" />

    <!-- Shrinks tokens into dmodel using a learned embedding -->
    <block title="Embedding">
        <import from="tokens" dim="[var(nvocab), var(ntokens)]" />
        <node op="MatMul">
            <params name="Embed" dim="[var(dmodel), var(nvocab)]" />
            <input src="tokens" dim="[var(nvocab), var(ntokens)]" />
            <output name="embeddings" />
        </node>
        <export from="embeddings" dim="[var(dmodel), var(ntokens)]" />
    </block>

    <!-- Not implemented - just an identity op -->
    <block title="PositionalEmbedding">
        <import from="embeddings" dim="[var(dmodel), var(ntokens)]" />
        <node op="Identity">
            <input src="embeddings" dim="[var(dmodel), var(ntokens)]" />
            <output name="posembeddings" />
        </node>
        <export from="posembeddings" dim="[var(dmodel), var(ntokens)]" />
    </block>

    <!-- The big decoder block -->
    <!-- NOT IMPLEMENTED YET -->
    <block title="DecoderLayer">
        <import from="posembeddings" dim="[[var(dmodel), var(ntokens)]" />
        <import from="scale" dim="[var(ntokens), var(ntokens)]" />
        <import from="mask" dim="[var(ntokens), var(ntokens)]" />
       
        <block title="Attention">
            <import from="posembeddings" dim="[var(dmodel), var(ntokens)]" />
            <import from="scale" dim="[var(ntokens), var(ntokens)]" />
            <import from="mask" dim="[var(ntokens), var(ntokens)]" />
            <block title="LinearQKV">
                <import from="posembeddings" dim="[var(dmodel), var(ntokens)]" />
                <node op="MatMul">
                    <params name="QueryWeights" dim="[var(dmodel), var(dmodel)]" />
                    <input src="posembeddings" />
                    <output name="queries" dims="[var(dmodel), var(ntokens)]" />
                </node>
                <node op="MatMul">
                    <params name="KeyWeights" dim="[var(dmodel), var(dmodel)]" />
                    <input src="posembeddings" />
                    <output name="keys" dims="[var(dmodel), var(ntokens)]" />
                </node>
                <node op="MatMul">
                    <params name="ValueWeights" dim="[var(dmodel), var(dmodel)]" />
                    <input src="posembeddings" />
                    <output name="values" dims="[var(dmodel), var(ntokens)]" />
                </node>
                <export from="queries" />
                <export from="keys" />
                <export from="values" />
            </block>
            <block title="ScaledDotProductAttention">
                <import from="scale" dim="[var(ntokens), var(ntokens)]" />
                <import from="mask" dim="[var(ntokens), var(ntokens)]" />
                <import from="queries" />
                <import from="values" />
                <import from="keys" />
                <node op="Transpose">
                    <input src="keys" />
                    <output name="keys_t" />
                </node>
                <node op="MatMul">
                    <input src="keys_t" />
                    <input src="queries" />
                    <output name="matmul" dim="[var(ntokens), var(ntokens)]" />
                </node>
                <node op="Div">
                    <input src="matmul" />
                    <input src="scale" />
                    <output name="scaled" />
                </node>
                <node title="Mask" op="Mul">
                    <input src="scaled" />
                    <input src="mask" />
                    <output name="masked" />
                </node>
                <node op="Softmax" axis="0">
                    <input src="masked" />
                    <output name="softmaxed" />
                </node>
                <node op="MatMul">
                    <input src="values" />
                    <input src="softmaxed" />
                    <output name="layer_out" dims="[var(dmodel), var(ntokens)]" />
                </node>
            </block>
            <export from="layer_out" />
        </block>

        <export from="layer_out" />
    </block>

    <!-- Linear to get to vocab size, no softmax - basically reverse of embedding -->
    <!-- Weights could be shared with embedding, but are not here -->
    <block title="Linear">
        <import from="posembeddings" />
         <node op="MatMul">
            <params name="RevEmbed" dim="[var(nvocab), var(dmodel)]" />
            <input src="posembeddings" dim="[var(dmodel), var(ntokens)]" />
            <output name="linear" dim="[var(nvocab), var(ntokens)]" />
        </node>
        <export from="linear" />
    </block>

    <!-- Final softmax to get probabilities -->
    <block title="Softmax">
        <import from="linear" />
        <!-- Note: softmax is along rows, i.e. columns will sum to 1 -->
        <node op="Softmax" axis="0">
            <input src="linear" dim="[var(nvocab), var(ntokens)]" />
            <output name="softmax" dim="[var(nvocab), var(ntokens)]" />
        </node>
        <export from="softmax" />
    </block>

    <export from="softmax" dim="[var(nvocab), var(ntokens)]" />
    <export from="layer_out" dim="[var(dmodel), var(ntokens)]" />

</model>